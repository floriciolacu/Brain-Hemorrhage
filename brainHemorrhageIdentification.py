# -*- coding: utf-8 -*-
"""BrainHemorrhageIdentification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SceqER_S08Dr6WJJAwuAWuFFF6je-K-P

# Brain Hemorrhage Identification

Conectarea la Google Drive pentru a prelua datele de acolo
"""

from google.colab import drive
drive.mount('/content/drive')

import cv2
import numpy as np
import tensorflow as tf

"""Citirea datelor (imaginilor si etichetelor pentru setul de antrenare). 
Schimbarea dimensiunii vectorului de etichete
"""

from keras.utils import to_categorical

images_path = "/content/drive/My Drive/DL/data/data/{}.png"
train_labels_path = "/content/drive/My Drive/DL/train_labels.txt"

train_data = open(train_labels_path)
train_data.readline()

features = []
labels = []

for dt in train_data:
  dt = dt[:-1].split(",")
  id = dt[0]
  label = int(dt[1])
  labels.append(label)
  img = cv2.imread(images_path.format(id))
  features.append(img)

features = np.array(features, dtype = np.float32)
labels = np.array(labels, dtype = np.float32)

labels = to_categorical(labels)

print("Gata")

"""Arhitectura modelului si optimizatorul. Antrenarea modelului"""

from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization
from keras.optimizers import Adam

model = Sequential()

model.add(Conv2D(128, (7, 7), activation = 'relu', input_shape = tuple(features.shape[1:])))
model.add(BatchNormalization())
model.add(MaxPooling2D())

model.add(Conv2D(64, (6, 6), activation = 'relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D())

model.add(Conv2D(32, (5, 5), activation = 'relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D())

model.add(Conv2D(16, (4, 4), activation = 'relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D())

model.add(Flatten())
model.add(Dense(1024, activation = 'relu'))
model.add(Dense(512, activation = 'relu'))
model.add(Dense(256, activation = 'relu'))
model.add(Dense(128, activation = 'relu'))
model.add(Dense(2, activation = 'softmax'))

adam = Adam(lr = 1e-5)
model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])

fit = model.fit(features, labels, epochs = 15, batch_size = 64)

"""Prezicerea etichetelor pentru imaginile din setul de test (5.149 imagini: de la 17.001 pana la 22.149) si scrierea lor in fisier"""

f = open("submissions.txt", "w")

f.write("id,class\n")

for i in range(17001, 22150):
  id = "0" + str(i)
  img_pred = cv2.imread(images_path.format(id))
  pred = model.predict(np.array([img_pred], dtype = np.float32)) 
  pred = pred[0]
  if pred[0] > pred[1]:
    pred = 0
  else:
    pred = 1
  f.write(str(id) + "," + str(pred) +"\n")

f.close()

"""### 3-Fold Cross Validation cu confusion matrix

Citirea datelor (la fel ca mai sus)
"""

from keras.utils import to_categorical

images_path = "/content/drive/My Drive/DL/data/data/{}.png"
train_labels_path = "/content/drive/My Drive/DL/train_labels.txt"

train_data = open(train_labels_path)
train_data.readline()

features = []
labels = []

for dt in train_data:
  dt = dt[:-1].split(",")
  id = dt[0]
  label = int(dt[1])
  labels.append(label)
  img = cv2.imread(images_path.format(id))
  features.append(img)

features = np.array(features, dtype = np.float32)
labels = np.array(labels, dtype = np.float32)

labels = to_categorical(labels)

print("Gata")

"""Arhitectura modelului si optimizatorul (la fel ca mai sus)"""

from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization
from keras.optimizers import Adam, SGD

def create_model(data_shape):
  K.clear_session()
  model = Sequential()

  model.add(Conv2D(128, (7, 7), activation = 'relu', input_shape = data_shape))
  model.add(BatchNormalization())
  model.add(MaxPooling2D())

  model.add(Conv2D(64, (6, 6), activation = 'relu'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D())

  model.add(Conv2D(32, (5, 5), activation = 'relu'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D())

  model.add(Conv2D(16, (4, 4), activation = 'relu'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D())

  model.add(Flatten())
  model.add(Dense(1024, activation = 'relu'))
  model.add(Dense(512, activation = 'relu'))
  model.add(Dense(256, activation = 'relu'))
  model.add(Dense(128, activation = 'relu'))
  model.add(Dense(2, activation = 'softmax'))

  adam = Adam(lr = 1e-5)
  model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])

  return model

"""Impartirea setului de date random in 3, crearea si antrenarea modelului pe fiecare din parti, calcularea pierderii si acuratetii si crearea matricei de confuzie"""

from sklearn.model_selection import KFold
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sn
import matplotlib.pyplot as plt

loss_acc = []

kf = KFold(n_splits =3, random_state=None, shuffle=True)
kf.get_n_splits(features)

for train_index, test_index in kf.split(features):
  model = create_model(tuple(features.shape[1:]))
  fit = model.fit(features[train_index], labels[train_index], epochs = 15, batch_size = 64)
  ev = model.evaluate(features[test_index], labels[test_index])
  loss_acc.append(ev)
  
  y_pred = model.predict(features[test_index])

  y_pred = np.argmax(y_pred, axis=1)
  y_exp = np.argmax(labels[test_index], axis=1)

  cm = confusion_matrix(y_exp, y_pred)
  
  sn.heatmap(cm, annot=True)
  plt.show()

print(loss_acc)